{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape DC Apartment Listings from ApartmentList.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1: 1500 Mass\n",
      "Page 1: Park Connecticut\n",
      "Page 1: 3003 Van Ness\n",
      "Page 1: 100K\n",
      "Page 1: 455 Eye Street\n",
      "Page 1: Connecticut Heights\n",
      "Page 1: The Flats at Dupont Circle\n",
      "Page 1: Alban Towers\n",
      "Page 1: Calvert Woodley\n",
      "Page 1: 2501 Porter\n",
      "Page 1: Cleveland House\n",
      "Page 1: Corcoran House at Dupont Circle\n",
      "Page 1: 425 Mass\n",
      "Page 1: 2400 M\n",
      "Page 1: 1210 Mass\n",
      "Page 1: RiverPoint\n",
      "Page 1: The Kelvin\n",
      "Page 1: Senate Square\n",
      "Page 1: 1331\n",
      "Page 1: 555\n",
      "Page 1: Resa\n",
      "Page 1: The Batley\n",
      "Page 1: 14W Apartments\n",
      "Page 1: The Channel\n",
      "Scrape Complete! Look in your current folder for results of the search.\n"
     ]
    }
   ],
   "source": [
    "# Load packages \n",
    "\n",
    "import requests  \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# # Many thanks to Ann Mohan Kunnath for the code to scrape Apartments.com\n",
    "# (https://towardsdatascience.com/an-introduction-to-web-scraping-with-python-bc9563fe8860)\n",
    "# It has been partially repurposed here for ApartmentList.com!\n",
    "\n",
    "# Read in some initial variables that we'll need to capture the data\n",
    "building_name = []\n",
    "building_address = []\n",
    "building_description = []\n",
    "building_units_avail = []\n",
    "apt_type = []\n",
    "apt_rent = []\n",
    "apt_footage = []\n",
    "apts_list = []\n",
    "\n",
    "\n",
    "# Base URL - apartments in Washington, DC\n",
    "base_url = 'https://www.apartmentlist.com/dc/washington'\n",
    "response = requests.get(base_url)\n",
    "if response.status_code == 200:\n",
    "    page = response.content\n",
    "    sopa = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    # Dynamically grab the last pagination number\n",
    "    paging = sopa.find('nav', {'class':'MuiPagination-root'}).find_all('li')\n",
    "    start_page = 1\n",
    "    last_page = str(paging[len(paging)-2].text)\n",
    "    \n",
    "    # Start working through each of the pages\n",
    "    for page_number in range(int(start_page),int(last_page) + 1):\n",
    "        url = '{}/page-{}'.format(base_url, str(page_number))\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:  \n",
    "            page = response.content\n",
    "            sopa = BeautifulSoup(page,\"html.parser\")    \n",
    "            \n",
    "            # Get building-level information\n",
    "            for listing in sopa.find_all('div', {'class', 'ListingCard'}): \n",
    "                building_name = listing.find('div', {'class', 'css-d6gmau e1k7pw6k4'}).text\n",
    "                building_address = listing.find('div', {'class', 'css-17xjl8p e1k7pw6k5'}).text\n",
    "                building_description = listing.find('div', {'class', 'css-11wmgwu e1k9ondy1'}).text\n",
    "                building_units_avail = listing.find('div', {'class', 'css-1qplh4f e1k7pw6k2'}).text\n",
    "\n",
    "                # Get apartment-level information\n",
    "                for apts in listing.find_all('div', {'class', 'css-1oxqqna e1i6tqc31'}):\n",
    "                    apt_type = apts.find('div', {'class', 'css-xjvzth e1i6tqc32'}).text\n",
    "                    apt_rent = apts.find('div', {'class', 'css-ajwnv4 e1i6tqc33'}).text\n",
    "                    if apts.find('div', {'class', 'css-o1qo1i e1i6tqc34'}):\n",
    "                        apt_footage = apts.find('div', {'class', 'css-o1qo1i e1i6tqc34'}).text\n",
    "                    else:\n",
    "                        apt_footage = 'NA'\n",
    "\n",
    "                    # Add this to a list\n",
    "                    apts_list.append([building_name,\n",
    "                                 building_address,\n",
    "                                 building_description,\n",
    "                                 building_units_avail,\n",
    "                                 apt_type,\n",
    "                                 apt_rent,\n",
    "                                 apt_footage])\n",
    "\n",
    "                # Print building name when done\n",
    "                print('Page {}: {}'.format(page_number, building_name))\n",
    "\n",
    "                # Give the site a chance to recover\n",
    "                time.sleep(2)\n",
    "\n",
    "# Make this a dataframe\n",
    "apts_df = pd.DataFrame(apts_list)\n",
    "apts_df.columns = ['building_name','building_address','building_description', 'building_units_available',\n",
    "                     'apt_type','apt_rent','apt_footage']\n",
    "\n",
    "# Read it out\n",
    "apts_df.to_csv(\"ApartmentList_Information.csv\")\n",
    "\n",
    "print('Scrape Complete! Look in your current folder for results of the search.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
