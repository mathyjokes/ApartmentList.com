{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cheers again to Brunna Torino for their great analysis of real estate prices in Amsterdam \n",
    "# (https://towardsdatascience.com/ai-and-real-state-renting-in-amsterdam-part-1-5fce18238dbc)\n",
    "# The ideas have been repurposed here to deal with the Washington, DC real estate market!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "apts = pd.read_csv('ApartmentList_Cleaned_Full.csv')\n",
    "apts_ml = apts[['apt_rent', 'apt_footage', 'rooms', 'distance_wh', 'distance_cap', 'building_units_available',\n",
    "               'adj_open', 'adj_large', 'adj_private', 'adj_great', 'adj_new', 'adj_beautiful', 'adj_high', 'adj_natural',\n",
    "               'adj_historic', 'adj_newly', 'adj_short', 'adj_ready', 'adj_furnished', 'adj_convenient', 'adj_windows']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Remove the two rows where apt_rent was still 0 (apartments with 10 and 11 bedrooms)\n",
    "apts_ml = apts_ml[apts_ml['apt_rent'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "target= np.array(apts_ml['apt_rent'])\n",
    "features = apts_ml.drop('apt_rent', axis = 1)\n",
    "feature_list = list(features.columns)\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 370.1\n",
      "Accuracy: 88.4 %.\n",
      "Mean Absolute Error: 270.51\n",
      "Accuracy: 89.47 %.\n",
      "Mean Absolute Error: 272.3\n",
      "Accuracy: 89.63 %.\n",
      "Mean Absolute Error: 336.65\n",
      "Accuracy: 87.93 %.\n",
      "Mean Absolute Error: 350.71\n",
      "Accuracy: 87.87 %.\n",
      "Mean Absolute Error: 259.46\n",
      "Accuracy: 90.22 %.\n",
      "Mean Absolute Error: 372.86\n",
      "Accuracy: 88.44 %.\n",
      "Mean Absolute Error: 329.09\n",
      "Accuracy: 87.67 %.\n",
      "Mean Absolute Error: 356.39\n",
      "Accuracy: 87.09 %.\n",
      "Mean Absolute Error: 396.05\n",
      "Accuracy: 85.48 %.\n",
      "Average accuracy: 88.22103844673418\n"
     ]
    }
   ],
   "source": [
    "## RANDOM FOREST - KFOLD AND MODEL \n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "accuracies = []\n",
    "for train_index, test_index in kf.split(features):\n",
    "\n",
    "    data_train   = features[train_index]\n",
    "    target_train = target[train_index]\n",
    "\n",
    "    data_test    = features[test_index]\n",
    "    target_test  = target[test_index]\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators = 1000, \n",
    "                               criterion = 'mse',\n",
    "                               bootstrap=True)\n",
    "    \n",
    "    rf.fit(data_train, target_train)\n",
    "\n",
    "    predictions = rf.predict(data_test)\n",
    "\n",
    "    errors = abs(predictions - target_test)\n",
    "\n",
    "    print('Mean Absolute Error:', round(np.mean(errors), 2))\n",
    "    \n",
    "    mape = 100 * (errors / target_test)\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "    print('Accuracy:', round(accuracy, 2), '%.')\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print('Average accuracy:', average_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "# That was alright - look at average accuracy of ~88.2. Let's do some hyperparameter tuning to get more accurate models\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 12.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1000, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 110, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "# Return the best parameters\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(data_train, target_train)\n",
    "\n",
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 354.78\n",
      "Accuracy: 87.31 %.\n",
      "Mean Absolute Error: 394.37\n",
      "Accuracy: 84.4 %.\n",
      "Mean Absolute Error: 360.24\n",
      "Accuracy: 86.83 %.\n",
      "Mean Absolute Error: 316.27\n",
      "Accuracy: 88.18 %.\n",
      "Mean Absolute Error: 372.18\n",
      "Accuracy: 87.26 %.\n",
      "Mean Absolute Error: 338.9\n",
      "Accuracy: 88.1 %.\n",
      "Mean Absolute Error: 340.74\n",
      "Accuracy: 86.26 %.\n",
      "Mean Absolute Error: 397.08\n",
      "Accuracy: 85.76 %.\n",
      "Mean Absolute Error: 391.36\n",
      "Accuracy: 86.7 %.\n",
      "Mean Absolute Error: 349.97\n",
      "Accuracy: 87.07 %.\n",
      "Average accuracy: 86.78790777518864\n"
     ]
    }
   ],
   "source": [
    "# Ok now let's include best parameters in the model\n",
    "# Best parameters from above: {'n_estimators': 1000, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 110, 'bootstrap': True}\n",
    "\n",
    "## RANDOM FOREST - KFOLD AND MODEL \n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "kf = KFold(n_splits=10,random_state=42,shuffle=True)\n",
    "accuracies = []\n",
    "for train_index, test_index in kf.split(features):\n",
    "\n",
    "    data_train   = features[train_index]\n",
    "    target_train = target[train_index]\n",
    "\n",
    "    data_test    = features[test_index]\n",
    "    target_test  = target[test_index]\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators = 200, \n",
    "                               random_state = 42, \n",
    "                               criterion = 'mse',\n",
    "                               min_samples_leaf = 1,\n",
    "                               min_samples_split= 2,\n",
    "                               max_features = 'sqrt',\n",
    "                               bootstrap=True,\n",
    "                               max_depth = 50)\n",
    "    \n",
    "    rf.fit(data_train, target_train)\n",
    "\n",
    "    predictions = rf.predict(data_test)\n",
    "\n",
    "    errors = abs(predictions - target_test)\n",
    "\n",
    "    print('Mean Absolute Error:', round(np.mean(errors), 2))\n",
    "    \n",
    "    mape = 100 * (errors / target_test)\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "    print('Accuracy:', round(accuracy, 2), '%.')\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print('Average accuracy:', average_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36873643095861935, 0.2596685572830047, 0.12583124555812933, 0.12551905682764444, 0.03299349540132837, 0.010862044972604793, 0.005520185200441033, 0.011333783357575402, 0.01088870692301239, 0.0067915641521334585, 0.006094640888575144, 0.009556393417439949]\n",
      "[(array([0], dtype=int64),), (array([1], dtype=int64),), (array([2], dtype=int64),), (array([3], dtype=int64),), (array([4], dtype=int64),), (array([5], dtype=int64),), (array([6], dtype=int64),), (array([9], dtype=int64),), (array([10], dtype=int64),), (array([11], dtype=int64),), (array([13], dtype=int64),), (array([17], dtype=int64),)]\n"
     ]
    }
   ],
   "source": [
    "# Ok, that bumped average accuracy up to 86.7%, which is definitely better. Now let's save that tree and see what itlooks like!\n",
    "## SAVING THE DECISION TREE \n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "tree = rf.estimators_[5]\n",
    "export_graphviz(tree, out_file = 'tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n",
    "(graph, ) = pydot.graph_from_dot_file('tree.dot')\n",
    "graph.write_png('tree.png')\n",
    "\n",
    "# Now let's take a look at feature importance\n",
    "y = rf.feature_importances_\n",
    "list_y = [a for a in y if a > 0.005]\n",
    "print(list_y)\n",
    "\n",
    "list_of_index = []\n",
    "for i in list_y:\n",
    "    a = np.where(y==i)\n",
    "    list_of_index.append(a)\n",
    "print(list_of_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also want to know which features were most important to the decisions\n",
    "\n",
    "# Get it in right format and add labels\n",
    "list_of_index = [0,1,2,3,4]\n",
    "col = []\n",
    "for i in feature_list:\n",
    "    col.append(i)\n",
    "labels = []\n",
    "for i in list_of_index:\n",
    "    b = col[i]\n",
    "    labels.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x612 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x612 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot most important features\n",
    "import matplotlib.pyplot as plt\n",
    "y = list_y\n",
    "fig, ax = plt.subplots() \n",
    "width = 0.8\n",
    "ind = np.arange(len(y)) \n",
    "ax.barh(ind, y,width, color=\"pink\")\n",
    "ax.set_yticks(ind+width/10)\n",
    "ax.set_yticklabels(labels, minor=False)\n",
    "plt.title('Feature importance in Random Forest Regression')\n",
    "plt.xlabel('Relative importance')\n",
    "plt.ylabel('feature') \n",
    "plt.figure(figsize=(10,8.5))\n",
    "fig.set_size_inches(10, 8.5, forward=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apt_rent</th>\n",
       "      <th>apt_footage</th>\n",
       "      <th>rooms</th>\n",
       "      <th>distance_wh</th>\n",
       "      <th>distance_cap</th>\n",
       "      <th>building_units_available</th>\n",
       "      <th>adj_open</th>\n",
       "      <th>adj_large</th>\n",
       "      <th>adj_private</th>\n",
       "      <th>adj_great</th>\n",
       "      <th>...</th>\n",
       "      <th>adj_beautiful</th>\n",
       "      <th>adj_high</th>\n",
       "      <th>adj_natural</th>\n",
       "      <th>adj_historic</th>\n",
       "      <th>adj_newly</th>\n",
       "      <th>adj_short</th>\n",
       "      <th>adj_ready</th>\n",
       "      <th>adj_furnished</th>\n",
       "      <th>adj_convenient</th>\n",
       "      <th>adj_windows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1427</td>\n",
       "      <td>456</td>\n",
       "      <td>1</td>\n",
       "      <td>933</td>\n",
       "      <td>2966</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1677</td>\n",
       "      <td>610</td>\n",
       "      <td>2</td>\n",
       "      <td>933</td>\n",
       "      <td>2966</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2078</td>\n",
       "      <td>762</td>\n",
       "      <td>2</td>\n",
       "      <td>5893</td>\n",
       "      <td>7877</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2876</td>\n",
       "      <td>1182</td>\n",
       "      <td>3</td>\n",
       "      <td>5893</td>\n",
       "      <td>7877</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2078</td>\n",
       "      <td>762</td>\n",
       "      <td>2</td>\n",
       "      <td>5893</td>\n",
       "      <td>7877</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>1311</td>\n",
       "      <td>480</td>\n",
       "      <td>1</td>\n",
       "      <td>1706</td>\n",
       "      <td>1734</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>1651</td>\n",
       "      <td>719</td>\n",
       "      <td>2</td>\n",
       "      <td>1706</td>\n",
       "      <td>1734</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>3135</td>\n",
       "      <td>1140</td>\n",
       "      <td>3</td>\n",
       "      <td>1706</td>\n",
       "      <td>1734</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>2200</td>\n",
       "      <td>575</td>\n",
       "      <td>2</td>\n",
       "      <td>4547</td>\n",
       "      <td>2063</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>10000</td>\n",
       "      <td>2636</td>\n",
       "      <td>6</td>\n",
       "      <td>2359</td>\n",
       "      <td>3046</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1995 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      apt_rent  apt_footage  rooms  distance_wh  distance_cap  \\\n",
       "0         1427          456      1          933          2966   \n",
       "1         1677          610      2          933          2966   \n",
       "2         2078          762      2         5893          7877   \n",
       "3         2876         1182      3         5893          7877   \n",
       "4         2078          762      2         5893          7877   \n",
       "...        ...          ...    ...          ...           ...   \n",
       "1992      1311          480      1         1706          1734   \n",
       "1993      1651          719      2         1706          1734   \n",
       "1994      3135         1140      3         1706          1734   \n",
       "1995      2200          575      2         4547          2063   \n",
       "1996     10000         2636      6         2359          3046   \n",
       "\n",
       "      building_units_available  adj_open  adj_large  adj_private  adj_great  \\\n",
       "0                           52         0          0            0          0   \n",
       "1                           52         0          0            0          0   \n",
       "2                           13         1          0            0          0   \n",
       "3                           13         1          0            0          0   \n",
       "4                           13         1          0            0          0   \n",
       "...                        ...       ...        ...          ...        ...   \n",
       "1992                         1         0          0            0          0   \n",
       "1993                         1         0          0            0          0   \n",
       "1994                         1         0          0            0          0   \n",
       "1995                         1         1          0            0          0   \n",
       "1996                         1         0          0            0          0   \n",
       "\n",
       "      ...  adj_beautiful  adj_high  adj_natural  adj_historic  adj_newly  \\\n",
       "0     ...              0         0            0             0          0   \n",
       "1     ...              0         0            0             0          0   \n",
       "2     ...              1         0            0             0          0   \n",
       "3     ...              1         0            0             0          0   \n",
       "4     ...              1         0            0             0          0   \n",
       "...   ...            ...       ...          ...           ...        ...   \n",
       "1992  ...              0         0            0             0          0   \n",
       "1993  ...              0         0            0             0          0   \n",
       "1994  ...              0         0            0             0          0   \n",
       "1995  ...              1         0            1             0          0   \n",
       "1996  ...              0         0            0             0          0   \n",
       "\n",
       "      adj_short  adj_ready  adj_furnished  adj_convenient  adj_windows  \n",
       "0             0          0              0               0            0  \n",
       "1             0          0              0               0            0  \n",
       "2             0          0              0               0            0  \n",
       "3             0          0              0               0            0  \n",
       "4             0          0              0               0            0  \n",
       "...         ...        ...            ...             ...          ...  \n",
       "1992          0          0              0               0            0  \n",
       "1993          0          0              0               0            0  \n",
       "1994          0          0              0               0            0  \n",
       "1995          0          0              0               0            0  \n",
       "1996          0          0              0               0            0  \n",
       "\n",
       "[1995 rows x 21 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apts_ml.to_csv('ApartmentList_ML.csv', index = False)\n",
    "apts_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the model to be used later!\n",
    "import pickle\n",
    "pickle.dump(rf, open('finalized_rf_model.sav', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
